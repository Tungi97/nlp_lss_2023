{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60bffde",
   "metadata": {},
   "source": [
    "# HW02: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed3f1a",
   "metadata": {},
   "source": [
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc19cf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters)</td>\n",
       "      <td>Reuters - Stocks ended slightly higher on Frid...</td>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              title  \\\n",
       "0  business  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "1  business    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "2  business  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "3  business  Oil prices soar to all-time record, posing new...   \n",
       "4  business        Stocks End Up, But Near Year Lows (Reuters)   \n",
       "\n",
       "                                                lead  \\\n",
       "0  Reuters - Private investment firm Carlyle Grou...   \n",
       "1  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "2  Reuters - Authorities have halted oil export\\f...   \n",
       "3  AFP - Tearaway world oil prices, toppling reco...   \n",
       "4  Reuters - Stocks ended slightly higher on Frid...   \n",
       "\n",
       "                                                text  \n",
       "0  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "1  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "2  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "3  Oil prices soar to all-time record, posing new...  \n",
       "4  Stocks End Up, But Near Year Lows (Reuters) Re...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7b626",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab1861a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel Acquires Chip Designers From HP (AP) AP - Intel Corp. has reached an agreement to hire hundreds of Hewlett-Packard Co. engineers who helped design the Itanium microprocessor, a massive joint project between the two technology companies since the early 1990s.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "dfs = df.sample(50)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "##TODO use spacy to split the documents in the sampled dataframe (dfs) in sentences and tokens\n",
    "docs = []\n",
    "for text in dfs[\"text\"]:\n",
    "    docs.append(nlp(text))\n",
    "\n",
    "##TODO print the first sentence of the first document in your sample\n",
    "print(next(docs[0].sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "65fa4c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel compound\n",
      "Acquires compound\n",
      "Chip compound\n",
      "designer nsubj\n",
      "from prep\n",
      "HP pobj\n",
      "( punct\n",
      "AP nmod\n",
      ") punct\n",
      "AP compound\n",
      "- punct\n",
      "Intel compound\n",
      "Corp. appos\n",
      "have aux\n",
      "reach ROOT\n",
      "an det\n",
      "agreement dobj\n",
      "to aux\n",
      "hire acl\n",
      "hundred dobj\n",
      "of prep\n",
      "Hewlett compound\n",
      "- punct\n",
      "Packard compound\n",
      "Co. compound\n",
      "engineer pobj\n",
      "who nsubj\n",
      "help relcl\n",
      "design xcomp\n",
      "the det\n",
      "Itanium compound\n",
      "microprocessor dobj\n",
      ", punct\n",
      "a det\n",
      "massive amod\n",
      "joint amod\n",
      "project appos\n",
      "between prep\n",
      "the det\n",
      "two nummod\n",
      "technology compound\n",
      "company pobj\n",
      "since prep\n",
      "the det\n",
      "early amod\n",
      "1990 pobj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "##TODO create a new column with tokens in lowercase (x.lower()), without punctuation tokens (x.is_punct), stopwords (x.is_stop), and digits (x.is_digit)\n",
    "\n",
    "def filterTokens(text):\n",
    "    doc = nlp(text)\n",
    "    filtered = []\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_stop and not token.is_digit:\n",
    "            filtered.append(token.lower_)\n",
    "    return filtered\n",
    "\n",
    "dfs[\"filtered\"] = dfs[\"text\"].apply(filterTokens)     \n",
    "    \n",
    "##TODO print the tokens (x.lemma_) and the dependency labels (x.dep_ ) of the first sentence of the first document (doc.sents)\n",
    "for token in next(docs[0].sents):\n",
    "    print(token.lemma_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef544f1",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Let's compute the ratio of named entities starting with a capital letter, e.g. if we have \"University of Chicago\" as a NE, \"University\" and \"Chicago\" are capitalized, \"of\" is not, thus the ratio is 2/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "be2cb111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RATIO: 0.7032136105860114\n"
     ]
    }
   ],
   "source": [
    "##TODO print the ratio of tokens being part of a named entity span starting with a capital letter (doc.ents)\n",
    "ent_len = 0\n",
    "num_upper = 0\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        ent_len += len(ent) # count number of tokens in entity\n",
    "        for token in ent:\n",
    "            if token.text[0].isupper():\n",
    "                num_upper += 1\n",
    "                \n",
    "print(\"RATIO: \" + str(num_upper / ent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "984b4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the ratio of capitalized tokens not being part of a named entity span (have no token.ent_type_)\n",
    "# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ede3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the ratio of capitalized tokens not being a named entity and not being the first token in a sentence\n",
    "# e.g. \"The dog barks\" = 0; 3 tokens, \"The\" is capitalized but the starting token of a sentence, no other tokens are capitalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466753be",
   "metadata": {},
   "source": [
    "Give an example of a capitalized token in the data which is neither a named entity nor at the start of a sentence. What could be the reason the token is capitalized (one sentence)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c0a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3126a5c7",
   "metadata": {},
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055edce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=0.01, \n",
    "                        max_df=0.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        use_idf=True, # the new piece\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##TODO using the whole sample, produce a world cloud with bigrams for label == business using tfidf frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948e50f",
   "metadata": {},
   "source": [
    "## Supervised Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "##TODO compute the number of words per document (excluding stopwords)\n",
    "##TODO get the most predictive features of the number of words per document using first f_class and then chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaee2e4",
   "metadata": {},
   "source": [
    "Are the results different? What could be a reason for this? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af74a1d",
   "metadata": {},
   "source": [
    "## Huggingface Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e75e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we use distilbert tokenizer\n",
    "# !pip install transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# let's instantiate a tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "##TODO tokenize the sentences in the sampled dataframe (dfs) using the DisilBertTokenizer\n",
    "##TODO what is the type/token ratio from this tokenizer (number_of_unqiue_token_types/number_of_tokens)?\n",
    "##TODO what is the amount of subword tokens returned by the huggingface tokenizer? hint: each subword token starts with \"#\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc9d00",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#TODO preprocess the corpus using spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ae310",
   "metadata": {},
   "source": [
    "### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_verb_pairs(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"nsubj\"]\n",
    "    pairs = [(w.lemma_.lower(), w.head.lemma_.lower()) for w in subjs]\n",
    "    return pairs\n",
    "##TODO extract the subject-verbs pairs and print the result for the second document\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c30e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same for verbs-object pairs ('dobj')\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad55039",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same for adjectives-nouns pairs ('amod')\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1273253",
   "metadata": {},
   "source": [
    "### Exploring cross label dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d19500",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO extract all the subject-verbs and verbs-object pairs for the verb \"rise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75649d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO for each label create a list ranking the most common subject-verbs pairs and one for the most common verbs-object pairs\n",
    "##TODO print the 10 most common pairs for each of the two lists for the labels \"world\" and \"sci/tech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d363b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d73fbafa1c1ce4c8cd6f07277ecbf2d62d7720c8bfdb6bdbaeeaf6dba6dd25dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
